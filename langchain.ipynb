{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required keys from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not HUGGINGFACE_API_KEY or not GROQ_API_KEY:\n",
    "    raise ValueError(\"Please set the environment variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Loading**\n",
    "\n",
    "Load the document using LangChainAI, a powerful tool for processing and analyzing large texts. This step enables the chatbot to access and understand the content, laying the foundation for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/korean_doc.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Splitting**\n",
    "\n",
    "Split the loaded content into smaller, overlapping chunks (or \"segments\") to maintain context and fit within the model's window size. This step ensures that the chatbot can parse and analyze the content efficiently, without losing important relationships between ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Embedding**\n",
    "Utilize the LaBSE (Language-agnostic BERT Sentence Embeddings) model from Hugging Face to generate vector embeddings, converting text data into numerical matrices that Large Language Models (LLMs) can understand. This step captures the semantic meaning and relationships within the content, enabling the chatbot to grasp nuances and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\OneDrive\\Documents\\Projects\\Open Source\\vectorDB\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\nikhi\\OneDrive\\Documents\\Projects\\Open Source\\vectorDB\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/LaBSE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Storing**\n",
    "Store the created embeddings in Epsilla's vector database, running locally in Docker. This step enables efficient and scalable data retrieval, allowing the chatbot to quickly access and process relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to localhost:8888 successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Epsilla\n",
    "from pyepsilla import vectordb as vector_db\n",
    "\n",
    "vectordb = vector_db.Client()\n",
    "\n",
    "vector_store = Epsilla.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    client=vectordb,\n",
    "    db_path=\"/vdb/kr\",\n",
    "    db_name=\"localDB\",\n",
    "    collection_name=\"KoreanDocCollection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Retrieval**\n",
    "Employ LangChainAI and the Llama-3.1 70B model through GroqInc to retrieve relevant data from the vector database and feed it into the LLM. This step generates a curated response, leveraging the chatbot's understanding of the content and context to provide accurate and informative answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatGroq(\n",
    "        api_key=GROQ_API_KEY,\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "    ),\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document appears to be a collection of questions and answers related to labor laws and employment regulations in India. It covers various topics such as:\n",
      "\n",
      "* Company employment rules and regulations\n",
      "* Labor laws and their impact on employer-employee relationships\n",
      "* Industrial safety and health regulations\n",
      "* Employment contracts and agreements\n",
      "* Termination of employment and severance pay\n",
      "* Employee rights and protections\n",
      "\n",
      "The document seems to be a guide or a resource for employers, HR professionals, or individuals seeking to understand the labor laws and regulations in India.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is this document about?\"\n",
    "result = qa_chain(question)\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be written in a mix of Korean and English, with some Korean text and some English phrases and sentences. However, the majority of the text seems to be in Korean.\n"
     ]
    }
   ],
   "source": [
    "question = \"which language is the document in?\"\n",
    "result = qa_chain(question)\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यह दस्तावेज़ भारत में श्रम कानूनों और विनियमों से संबंधित है, विशेष रूप से कर्मचारियों की छंटनी और उनके अधिकारों के बारे में बताता है। इसमें कर्मचारियों को दी जाने वाली सेवा समाप्ति की सूचना के बारे में जानकारी दी गई है, जिसमें कर्मचारी को दी जाने वाली जानकारी और कंपनी की संपत्ति की वापसी की प्रक्रिया शामिल है।\n"
     ]
    }
   ],
   "source": [
    "question = \"इस दस्तावेज़ के बारे में मुझे संक्षिप्त विवरण दें\"\n",
    "result = qa_chain(question)\n",
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
